---
layout: post
title: Emulating Marxan in R
published: true
excerpt: >
  Marxan is the most widely used software for the systematic design of protected
  areas. In this post, I dealve into the details of Marxan and emulate it in R.
category: r
tags: r marxan conservation
---

```{r echo = F, include = F, eval = F}
setwd("_source/")
```

More than 1/8th of Earth's land area is within some form of [protected area](http://www.protectedplanet.net/); however, protected area (PA) networks often do a poor job of representing the biodiversity that they ostensibly protect. In large part, this is because new parks and PAs historically haven't been placed in the optimal locations for conservation. Often they are placed opportunistically, where the land is cheap and unlikely to experience land use change anyway. Many of the most famous parks in North America (e.g. Yosemite or Banff) were chosen more for scenic and recreation value than conservation potential. Or, more subtly, PAs may not complement each other, with some taxa or habitats overrepresented and others underrepresented.

Enter the field of Systematic Conservation Planning, which offers a set of tools and techniques for systematically identifying the optimal location of new PAs in a way that maximizes conservation benefit and minimizes socio-economic cost. These tools have changed the way conservation decisions are made, and how and where conservation resources are allocated. [Marxan](http://uq.edu.au/marxan/) is the most widely used tool for systematic reserve design. It's used globally in both the terrestrial and marine realms, for projects large and small.

A detailed understanding of how Marxan works isn't required to use the software; however, I hate a black box so, as I started using Marxan, I began digging into the documentation and source code to see exactly what it was doing. This led me to emulating Marxan within R as a means of checking that I was understanding everything correctly. In addition, this R Marxan emulator offers a nice platform for prototyping changes to Marxan without having to deal with the C source code. Of course, Marxan is highly optimized in C and very feature rich, and this R emulator could never be used in a real planning exercise. But, I present it here in the hopes that it may be instructive.

# Required packages

```{r}
library(dplyr)
library(tidyr)
library(assertthat)
library(sp)
library(raster)
library(rgeos)
library(rasterVis)
library(viridis)
library(gstat)
library(marxan)
set.seed(1)
```

# Background

## Resources

To write this post, I used the following resources:

- The [Marxan User Manual](http://www.uq.edu.au/marxan/docs/Marxan_User_Manual_2008.pdf)
- The [Marxan Good Practices Handbook](http://www.uq.edu.au/marxan/docs/Marxan%20Good%20Practices%20Handbook%20v2%202010.pdf)
- The book [Spatial Convervation Prioritization](https://global.oup.com/academic/product/spatial-conservation-prioritization-9780199547777)
- The [Marxan source code](https://github.com/mattwatts/marxan244) on GitHub
- The C source code for R's `optim` function, which implements simulated annealing

## Marxan

A Marxan exercise starts by dividing the study region into planning units (typically square or hexagonal cells) and, for each planning unit, assigning values that quantify socio-economic cost and conservation benefit for a set of conservation features. The **cost** can be the acquisition cost of the land, the costs of management, the opportunity cost of foregone commercial activities (e.g. from logging or oil palm development), or simply the area. The **conservation features** are typically species (e.g. Clouded Leopard) or habitats (e.g. mangroves or cloud forest). The benefit that each features derives from a planning unit can take a variety of forms, but is typically either occupancy (i.e. presence or absence) or area of occurence within each planning unit. Finally, for each conservation feature, representation targets must be set, such as 20% of the historical extent of cloud forest or 10,000km<sup>2</sup> of Clouded Leopard habitat.

The ultimate goal of a conservation planning exercise is to ensure the long term **persistence** of the conservation features. When planning for persistence, it's important to consider the total area of protected habitat *and* the spatial configuration of that habitat. A reserve network that is highly [fragmented](https://en.wikipedia.org/wiki/Habitat_fragmentation) with poor [connectivity](https://en.wikipedia.org/wiki/Landscape_connectivity) between patches will not be able to support species in the long term. To account for this, Marxan can be configured to look for more compact reserve networks by minimizing the total boundary length of the reserve.

Given these inputs, Marxan finds the set of planning units that meets the representation targets while minimizing cost and boundary length. This is a form of the [**minimum set cover problem**](https://en.wikipedia.org/wiki/Set_cover_problem) and is formulated mathematically, for \\( n \\) planning units and \\( m \\) conservation features, as:

$$
\text{Minimize} \sum_{i=1}^{n}x_i c_i + 
b \sum_{i=1}^{n} \sum_{j=1}^{n}x_i (1-x_j)\nu_{ij}
\text{ subject to } \sum_{i=1}^{n}x_i r_{ij}
\geq T_j \space \forall \space j 
$$

where \\( x_i \\) is a binary decision variable specifying whether a planning unit has been selected (1) or not (0), \\( c_i \\) is the cost of planning unit \\( i \\), \\( \nu_{ij} \\) is a matrix of boundary lengths between planning units, \\( r_{ij} \\) is the representation level of feature \\( j \\) in planning unit \\( i \\), and \\( T_j \\) is the target for feature \\( j \\). Finally, $b$ is known as the **Boundary Length Modifier (BLM)** and determines how much emphasis should be placed on producing compact and connected solutions relative to meeting targets and minimizing cost. Note that the second term, with the double summation, is just the total length of the external boundary of the reserve network.

Finally, a status code can be assigned to each planning unit to specify whether it is locked in or locked out the of solution. The codes and their meanings are as follows:

|Status|Meaning                                                             |
|:-----|:-------------------------------------------------------------------|
|0     |Not guaranteed in initial reserve system                            |
|1     |Included in initial reserve system, but not guaranteed in final     |
|2     |Locked in to solution, i.e. guaranteed protected                    |
|3     |Lockout out of solution, i.e. guaranteed unprotected                |

Marxan requires 5 input files in order to run:

1.  Planning Units (pu.dat): units to be considered for the reserve system, including cost and status.
2.  Conservation Features (spec.dat): features and corresponding targets.
3.  Planning Units vs. Conservation Features (puvspr2.dat): representation level of each feature within each planning unit.
4.  Boundary Length (bound.dat): pairwise shared boundaries between planning units.
5.  Parameters (input.dat): parameters and settings for the Marxan run.

The [`marxan`](https://github.com/paleo13/marxan) R package acts as an interface between R and Marxan. It automates the process of generating these input files from R objects and provides an R wrapper for the Marxan executable.

## Simulated Annealing

The main method that Marxan uses for solving reserve design problems is **simulated annealing**, a stochastic heuristic for approximating global optima of complex functions with many local optima. Simulated annealing is a highly flexible optimization method that that minimizes a given **objective function** over a set of **decision variables** by stochastically exploring the state space of the decision variables. Starting from a randomly generated initial state, simulated annealing proceeds iteratively by:

1.  Randomly picking a neighbouring state from a well-defined set of candidates, and calculating the value of the objective function at this state.
2.  If the candidate state reduces the objective function, accept it. If the candidate state increases the objective function, accept it according to an **acceptance probability**, which depends on the change in objective function value and a global **temperature** parameter.
3.  As the heuristic progresses, the temperature parameter gradually decreases according to an **annealing schedule**. As a result, changes that increase the objective function become less likely to be accepted. Initially permitting "bad" changes ensures that the heuristic is unlikely to get caught in a local minimum.

Simulated annealing does not find the true minimum of the objective function. Rather it finds a near optimal solution and, since it is stochastic, each time simulated annealing is run a different near optimal solution is returned.

In the context of Marxan, the decision variable is a binary vector indicating which planning units are included in the reserve, and the states are different reserve configurations. Choosing a neighbouring state involves randomly selecting a planning unit and switching its status from selected to not selected or vice versa. In a typical Marxan run, simulated annealing is performed many times to generate a suite of possible reserve networks. Thus, multiple options can be presented to decision makers, and the importance of particular planning units can be measured by the selection frequency (i.e. number of solutions containing that planning unit).

# Example data

First I create some very simple example data: 4 species distributions, a cost layer, and a grid of 25km<sup>2</sup> planning units, all defined on a 100km x 100km study area.

## Conservation features

I create 4 artificial 1km resolution raster layers that I'll use to represent species distributions. To work with data that are at least somewhat realistic I introduce some autocorrelation into the layers by [generating spatially autocorrelated Gaussian random fields](http://santiago.begueria.es/2010/10/generating-spatially-correlated-random-fields-with-r/). I also give 2 different characteristic scales for the species and 2 different levels of rarity.

```{r features, results='hide'}
# raster template
utm10 <- crs('+proj=utm +zone=10 +ellps=GRS80 +datum=NAD83 +units=km +no_defs')
r <- extent(c(0, 100, 0, 100)) %>% 
    raster(nrows = 100, ncols = 100, crs = utm10, vals = 1)

gaussian_field <- function(r, range, pct, beta = c(1, 0, 0)) {
  gsim <- gstat(formula = (z ~ x + y), dummy = TRUE, beta = beta, nmax = 20,
               model = vgm(psill = 1, range = range, model = 'Exp'))
  vals <- rasterToPoints(r, spatial = TRUE) %>% 
    geometry %>% 
    predict(gsim, newdata = ., nsim = 1) %>% 
    {scale(.$sim1)}
  if (!missing(pct)) {
    vals <- as.integer(vals < quantile(vals, pct))
  }
  r[] <- vals
  r
}

# conservation features
species <- mapply(function(x, y, r) gaussian_field(r = r, range = x, pct = y, 
                                                   beta = c(1, 0.02, 0.01)),
                  rep(10, 4), c(0.01, 0.05, 0.1, 0.15),
                  MoreArgs = list(r = r)) %>% 
  stack %>% 
  setNames(letters[1:4])
levelplot(species, main='Feature Distribution', layout = c(2, 2),
          scales=list(draw=FALSE),
          col.regions = c("grey20", "#fd9900"), colorkey = FALSE)
```

## Cost

I use a similar approach to generating a spatially autocorrelated cost layer.

```{r cost, results='hide'}
cost <- gaussian_field(r, 10, beta = c(1, 0, 0)) %>% 
  stretch(10, 1000) %>% 
  setNames("cost")
levelplot(cost, main = "Cost", margin = FALSE,
          col.regions = viridis, at = seq(0, 1000, len = 256))
```

## Planning units

```{r make-grid, echo=FALSE}
make_grid <- function(x, type, cell_width, cell_area, clip = FALSE) {
  if (!type %in% c("square", "hexagonal")) {
    stop("Type must be either 'square' or 'hexagonal'")
  }
  
  if (missing(cell_width)) {
    if (missing(cell_area)) {
      stop("Must provide cell_width or cell_area")
    } else {
      if (type == "square") {
        cell_width <- sqrt(cell_area)
      } else if (type == "hexagonal") {
        cell_width <- sqrt(2 * cell_area / sqrt(3))
      }
    }
  }
  # buffered extent of study area to define cells over
  ext <- as(extent(x) + cell_width, "SpatialPolygons")
  projection(ext) <- projection(x)
  # generate grid
  if (type == "square") {
    g <- raster(ext, resolution = cell_width)
    g <- as(g, "SpatialPolygons")
  } else if (type == "hexagonal") {
    # generate array of hexagon centers
    g <- spsample(ext, type = "hexagonal", cellsize = cell_width, offset = c(0, 0))
    # convert center points to hexagons
    g <- HexPoints2SpatialPolygons(g, dx = cell_width)
  }
  
  # clip to boundary of study area
  if (clip) {
    g <- gIntersection(g, x, byid = TRUE)
  } else {
    g <- g[x, ]
  }
  # clean up feature IDs
  row.names(g) <- as.character(1:length(g))
  return(g)
}
```

In a [previous post](http://strimas.com/spatial/hexagonal-grids/), I discussed the merits of hexagonal grids and defined a funtion to create such grids. I now use that function to generate a grid (see [previous post](http://strimas.com/spatial/hexagonal-grids/) for definition of `make_grid`). Since I'll be performing the simulated annealing within R, I want to keep the parameter space small so the problem is feasible. Therefore, I divide the study area up in to just 378 25km<sup>2</sup> planning units.

For completeness I randomly assign a patch of 7 planning units to be locked in and a patch of 7 to be locked out. Locked in planning units are typically those already within protected areas, while locked out planning units could be developed for another land use (e.g. urban or plantation) and therefore be unavailable for protection.

```{r pus}
sa_border <- as(extent(r), "SpatialPolygons")
projection(sa_border) <- projection(r)
pu <- make_grid(sa_border, "hexagonal", cell_area = 25, clip = FALSE)
pu <- pu[gContains(sa_border, pu, byid = TRUE), ]
pu$id <- 1:length(pu)
row.names(pu) <- as.character(pu$id)
pu$status <- 0L
# randomly create patches with status = 2 and 3
inner_cells <- gContainsProperly(gUnaryUnion(pu), pu, byid = TRUE) %>% 
  which %>% 
  sample(2)
locked_in <- gIntersects(pu[inner_cells[1],], pu, byid = TRUE)
pu$status[locked_in] <- 2L
locked_out <- gIntersects(pu[inner_cells[2],], pu, byid = TRUE)
pu$status[locked_out] <- 3L
```

Now I average the cost raster layer over the hexagonal planning units.

```{r pu-cost}
pu <- raster::extract(cost, pu, fun = mean, na.rm = TRUE, sp = TRUE)
spplot(pu, "cost", main = "Planning Unit Cost", col.regions = viridis(256),
       at = seq(floor(min(pu$cost)), ceiling(max(pu$cost)), len = 256))
```

# Implementation in R

Now that I've given an overview of the problem, and generated some data, it's time to explore the details of Marxan and reconstruct them in R. I break down each component in turn, defining functions for each task.

## Pre-processing

Some pre-processing needs to be done to get the spatial data into a form useful for simulated annealing.

### Feature representation

The spatial distribution of features over the landscape typically comes in the form of a series of raster layers. Here I create a function that summarizes these layers to a data frame of species representation within each planning unit, which is essentially the \\( r_{ij} \\) term in the above objective function and the `puvspr.dat` Marxan input file.

```{r sp-occ}
summarize_features <- function(features, pu) {
  raster::extract(features, pu, fun = sum, na.rm = TRUE, sp = FALSE) %>% 
    data.frame(pu = pu$id, .) %>% 
    gather(feature, amount, -pu)
}
puvspr <- summarize_features(species, pu)
head(puvspr, 10) %>% 
  kable
```

### Setting targets and SPF

Prior to a Marxan run, absolute representation targets for each conservation feature must be set. This may ocur through expert elicitation, modeling exercise to determine the amount of habitat necessary to ensure persistence, or more informal methods. Regardless, it's very common to set proportional targets, such as protected 20% of the range of a given species or habitat type. I define a function that converts proportional targets to absolute representation targets.

In addition, this function sets the Species Penalty Factor (SPF) for each feature. I will explain these in detail below but, for now, they can be thought of as measuring the relative importance of meeting the representation target for each feature.

The data frame output by this function corresponds to the `spec.dat` Marxan input file.

```{r target-setting}
prepare_features <- function(rij, target_prop, spf = 1) {
  assert_that(length(target_prop) == 1 || is.data.frame(target_prop),
              length(spf) == 1 || is.data.frame(spf))
  
  rij_sum <- group_by(rij, feature) %>% 
    summarize(amount = sum(amount))
  
  # set targets
  if (is.data.frame(target_prop)) {
    assert_that("feature" %in% names(target_prop),
                all(rij_sum$feature %in% target_prop$feature))
    features <- inner_join(rij_sum, target_prop, by = "feature") %>% 
      mutate(target = prop * amount)
  } else {
    features <- mutate(rij_sum, prop = target_prop, target = prop * amount)
  }
  
  # set spf
  if (is.data.frame(spf)) {
    assert_that("feature" %in% names(spf),
                all(target_prop$feature %in% spf$feature))
    features <- inner_join(features, spf, by = "feature")
  } else {
    features <- mutate(features, spf = spf)
  }
  dplyr::select(features, feature, prop, total_amount = amount, target, spf)
}
# 20% target for all species 
prepare_features(puvspr, 0.2, 10) %>% 
  kable
# species specific targets
ex_targets <- data_frame(feature = letters[1:4],
                         prop = rep(c(0.50, 0.15), each = 2))
ex_spf <- data_frame(feature = letters[1:4], spf = 1:4)
spec <- prepare_features(puvspr, ex_targets, ex_spf)
kable(spec)
```

## Calculate shared boundaries

In addition to minimizing cost, Marxan can also be instructed to minimize the length of the external boundary of the reserve network. This external boundary can be calculated explicitely at each simulated annealing iteration; however, it's more efficient to calculate a dataframe of all the pairwise shared boundaries once, then use this to calculate the overall boundary length at each iteration.

```{r boundaries}
calculate_boundary <- function(pu) {
  row.names(pu) <- as.character(pu$id)
  # calculate all boundaries
  boundary <- geometry(pu) %>% 
    as("SpatialLines") %>% 
    gIntersection(., ., byid = TRUE) %>% 
    gLength(byid = TRUE) %>% 
    data_frame(pid = names(.), boundary = .) %>% 
    separate(pid, c("id1", "id2"), sep = " ") %>% 
    mutate(id1 = as(id1, class(pu$id)), id2 = as(id2, class(pu$id))) %>% 
    filter(id1 <= id2)
  min_boundary <- 1e-6 * min(boundary$boundary)
  # internal boundaries
  internal_boundary <- filter(boundary, id1 != id2) %>% 
    rbind(., mutate(., id1 = id2)) %>% 
    group_by(id1) %>% 
    summarize(internal = sum(boundary))
  # subtract internal boundaries from boundary of pu with itself
  # to get external boundary
  external_boundary <- filter(boundary, id1 == id2) %>% 
    left_join(internal_boundary, "id1") %>% 
    mutate(boundary = (boundary - ifelse(is.na(internal), 0, internal))) %>% 
    dplyr::select(-internal) %>% 
    mutate(boundary = ifelse(boundary < min_boundary, 0, boundary))
  boundary <- filter(boundary, id1 != id2) %>% 
    rbind(external_boundary) %>% 
    arrange(id1, id2)
  return(boundary)
}
boundary <- calculate_boundary(pu)
head(boundary, 10) %>% 
  kable(digits = 3)
```

## Simulated annealing

Next, I'll delve into simulated annealing, which I introduced above. The function `optim` in the `stats` package provides implementations for a variety of optimization methods, including simulated annealing. For efficiency, each of the optimization methods is actually implemented in C and called via R. The [C source code](https://github.com/wch/r-source/blob/b156e3a711967f58131e23c1b1dc1ea90e2f0c43/src/appl/optim.c) is viewable on GitHub.

### Annealing schedule

Recall that the key to simulated annealing is that "bad" changes are accepted with some probability that declines as the heuristic progresses. This acceptance probability is given by:

$$
\min\left \{ 1,\exp \left (-\frac{f(\vec{x}_{n+1}) - f(\vec{x}_n)}{T_m} \right ) \right \}
$$

where \\( \\vec{x}_n \\) is the binary decision variable at annealing iteration \\( n \\) and \\( T_m \\) is the temperature parameter after \\( m \\) decreases. The two different indices, \\( n \\) and \\( m \\), occur because, at each temperature, there are typically multiple annealing iterations performed. The **annealing schedule** determines the rate at which temperature decreases, and alternate implementations of simulated annealing use different functional forms for the schedule. In partcular, Marxan uses

$$
T_m = \alpha^m * T_0
$$

and `optim` uses

$$
T_m = \frac{T_0}{\log(m + e)}
$$

where \\( T_0 \\) is the starting temperature and \\( \\alpha \\) is a cooling factor, both of which are user-defined parameters.

### Vanilla R implementation

Since `optim` uses a different annealing schedule, and the inner workings are obscured because it uses C, I implement my own Marxan-like simulated annealing in plain R.

```{r sa}
anneal <- function(x_init, objective, neighbour, n, ntemp, t_init, alpha) {
  assert_that(is.count(n), is.count(ntemp),
              ntemp > 1, ntemp <= n, 
              n %% ntemp == 0)
  
  n_per_temp <- floor(n / ntemp)
  t <- t_init
  x <- x_init
  for (i in 1:n) {
    if (i %% n_per_temp == 0) {
      # decrease temperature
      t <- alpha * t
    }
    x_try <- neighbour(x)
    delta <- objective(x_try) - objective(x)
    if (delta <= 0) {
      # "good" change
      x <- x_try
    } else if (runif(1) < exp(-delta / t)) {
      # "bad" change
      x <- x_try
    }
  }
  return(x)
}
```

In this implementation, `objection` and `neighbour` are the objective function and neighbour selection function respectively. `x_init` is a vector of initial values for the decision variable, `n` is the number of iterations, `ntemp` is the number of temperature decreases, `t_init` is the starting temperature (\\( T_0 \\)), and `alpha` is the cooling factor (\\( \\alpha \\)).

To keep this simulated annealing implementation as flexible as possible, the only arguement the objective and neighbour selection functions take is the vector of decision variables. In general, other data or parameters will be required to evaluate these functions, but this additional information must be stored within the enclosing environment of the function. This can be accomplished using **closures** functions created by other functions, which have access to all variables in the parent function. This will become more clear when I define these closure below. The [Advanced R](http://adv-r.had.co.nz/) book by Hadley Wickham has great coverage of this topic, particularly the sections on [function environments](http://adv-r.had.co.nz/Environments.html#function-envs) and [closures](http://adv-r.had.co.nz/Functional-programming.html#closures)

## Initial reserve system

Simulated annealing needs to start somewhere. Marxan randomly picks a user-defined proportion of planning units to be included in the initial reserve system. I defined a function that, given an set of planning units and a proportion, returns a logical vector indictating which units are selected for the inital reserve. In addition, the planning unit status described above is taken into account when choosing which units to include in the initial reserve.

```{r initial}
init_res <- function(pu, prop = 0) {
  n <- round(prop * nrow(pu))
  # account for locked in planning units
  x <- pu$status %in% 1:2
  if (sum(x) >= n) {
    return(x)
  }
  # add more to meet specified proportion
  n <- n - sum(x)
  add <- which(pu$status == 0) %>% 
    sample(n)
  x[add] <- TRUE
  return(x)
}
sum(init_res(pu, 0.5)) / length(pu)
```

## Neighbour selection function

At each iteration, simulated annealing must have some means of moving from the current candidate state to a neighbouring candidate state In the case of Marxan, this is as simple as switching the binary decision variable for a single, randomly chosen plannin unit. Again, planning unit status must be taken into account since some units must be excluded.

As noted above, the neighbour function must be a closure taking just one arguement: the vector of decision variables. The list of planning units that can be switched (i.e. those with status 0 or 1) is stored in the enclosing environment. To make this all happen, I define a generator function for Marxan-like neighbour selection closures.

```{r neighbour}
generate_nsf <- function(pu) {
  x_valid <- (1:nrow(pu))[pu$status %in% 0:1]
  rm(pu)
  function(x) {
    i <- sample(x_valid, 1)
    x[i] <- !x[i]
    return(x)
  }
}
ex_nsf <- generate_nsf(pu)
ex_res <- init_res(pu, 0)
neigh <- ex_nsf(ex_res)
sum(ex_res - neigh)
```

## Objective function

Recall that the problem Marxan seeks to solve can be formulated mathematically in terms of the minimization of an objective function subject to the constraint that all representation targets are met:

$$
\text{Minimize} \sum_{i=1}^{n}x_i c_i + 
b \sum_{i=1}^{n} \sum_{j=1}^{n}x_i (1-x_j)B_{ij}
\text{ subject to } \sum_{i=1}^{m}x_i r_{ij}
\geq T_j \space \forall \space j
$$

The two terms in the objective function correspond to minimizing the cost and external boundary of the reserve network, respectively. Rather than strictly enforcing the representation constraint, Marxan incorporates the constraint into the objective function as a **shortfall penalty**. This approach is more efficient because the constraint does not have to be enforced in each simulated annealing iteration. Furthermore, it allows more flexibility within simulated annealing, by temporarily allowing solutions that don't meet targets. The unconstrained objective function that Marxan actually optimizes is:

$$
f(\vec{x}) = \sum_{i=1}^{n}x_i c_i + 
b \sum_{i=1}^{n} \sum_{j=1}^{n}x_i (1-x_j)\nu_{ij} +
\sum_{j=1}^{m}s_j \gamma_j\cdot 
\max\left( 1 - \frac{1}{T_j}\sum_{i=1}^{n}x_i r_{ij},0\right)
$$

The final term is the shortfall penalty. For each feature for which the target isn't met, this penalty is proportional to the relative shortfall to the target. The parameter \\( s_j \\) is the **Species Penalty Factor (SPF)**, which determines the magnitude of the penalty to apply if the target isn't met for a given feature. A larger value of the SPF will put more emphasis on meeting targets for that feature relative to minimizing cost and boundary length. Finally, \\( \\gamma_j \\) is the base shortfall penalty, which essentially acts to convert the shorfall penalty into units of cost. Marxan calculates these values as the cost of meeting a given feature's target in the cheapest way possible, ignoring all other features This results in the shortfall penalty being an approximation of the cost required to raise a feature's level of representation to the target level.

### Base shortfall penalty

Marxan uses a simple greedy agorithm for estimating \\( \\gamma_j \\). For each feature, planning units are ranked in descending order of efficiency, i.e. the ratio of representation level for that feature and cost (economic cost plus boundary length). Planning units are then added in order until the taret is met, and the resulting cost is \\( \\gamma_j \\). This doesn't guarantee the cheapest way to meet a feature's target, but it give a good approximation.

The approach described in the previous paragraph is what the [Marxan User's Manual](http://www.uq.edu.au/marxan/docs/Marxan_User_Manual_2008.pdf) *says* Marxan is doing; however, looking at the [source code](https://github.com/mattwatts/marxan244), there's an additional wrinkle. Rather that just adding planning units in order of efficiency, Marxan will also look to see if, at any point, the addition of a single planning unit will result in the target being met. If so, and the cost of this planning unit is lower than the cost of the most efficient unit, than this planning unit is chosen instead of the most efficient one.

```{r base-penalty}
base_penalty <- function(pu, rij, features, blm) {
  # calculate boundary of each planning unit
  pu$boundary <- gLength(pu, byid = TRUE)
  # calculate efficiency = representation / cost for each planning unit
  remaining <- as.data.frame(pu) %>% 
    mutate(total_cost = (cost + blm * boundary)) %>% 
    inner_join(rij, ., by = c("pu" = "id")) %>% 
    filter(amount != 0, status != 3) %>% 
    mutate(efficiency = amount / total_cost)
  # start by including free planning units and those with status = 2
  p <- filter(remaining, status == 2 | total_cost == 0) %>% 
    group_by(feature) %>% 
    summarize(penalty = sum(total_cost), 
              cumulative_amount = sum(amount)) %>% 
    right_join(dplyr::select(features, feature, target), by = "feature") %>% 
    mutate(shortfall = (target - cumulative_amount),
           shortfall = ifelse(is.na(shortfall), target, shortfall),
           penalty = ifelse(is.na(penalty), 0, penalty)) %>% 
    dplyr::select(feature, penalty, shortfall)
  remaining <- filter(remaining, status != 2, total_cost != 0)
  for (f in p$feature) {
    # find and add next best planning unit
    r <- filter(remaining, feature == f) %>% 
      arrange(desc(efficiency))
    while(p$shortfall[p$feature == f] > 0 && nrow(r) > 0) {
      best <- head(r, 1)
      meets_target <- r %>% 
        filter(amount >= p$shortfall[p$feature == f],
               total_cost < best$cost) %>% 
        arrange(total_cost) %>% 
        head(1)
      if (nrow(meets_target) > 0) {
        best <- meets_target
      }
      p$shortfall[p$feature == f] <- p$shortfall[p$feature == f] - best$amount
      p$penalty[p$feature == f] <- p$penalty[p$feature == f] + best$cost
    }
  }
  dplyr::select(p, feature, penalty)
}
base_penalty(pu, puvspr, spec, blm = 0) %>% 
  kable(digits = 1)
```

### Objective function implementation

As with the neighbour selection function defined earlier, I implement the objective function as a closure that takes the decision variable as an arguement and store all other required data and parameters in the enclosing environment. Again, to make this happen, I define a generator function for Marxan-like objective function closures.

```{r obj-func}
generate_objective <- function(pu, rij, features, boundary, blm = 0) {
  # calculate base shortfall penalties
  features <- base_penalty(pu = pu, rij = rij, features = features, blm = blm) %>% 
    inner_join(features, by = "feature")
  function(x) {
    if (all(!x)) {
      cost <- 0
      boundary <- 0
      shortfall <- sum(targets$spf * features$penalty)
    } else {
      pu_selected <- pu[x,]
      # soci-economic cost
      cost <- sum(pu_selected$cost)
      # boundary length
      if (blm == 0) {
        boundary_cost <- 0
      } else {
        boundary_cost <- filter(boundary, 
            (id1 %in% pu_selected$id & !id2 %in% pu_selected$id) |
            (id2 %in% pu_selected$id & !id1 %in% pu_selected$id)) %>% 
          {sum(.$boundary)}
      }
      # shortfall penalty
      shortfall <- data_frame(pu = pu_selected$id) %>% 
        inner_join(rij, by = "pu") %>% 
        group_by(feature) %>% 
        summarize(amount = sum(amount)) %>% 
        left_join(features, by = "feature") %>% 
        mutate(shortfall = spf * penalty * max(1 - amount / target, 0)) %>% 
        {sum(.$shortfall)}
    }
    return(cost + (blm * boundary_cost) + shortfall)
  }
}
ex_objective <- generate_objective(pu, puvspr, spec, boundary, blm = 0)
ex_objective(init_res(pu, 0.5))
```

## Adaptive annealing

In the simulated annealing formulation Marxan uses, there are two parameters that define the annealing schedule: \\( T_0 \\) is the initial temperature and \\( \\alpha \\) is the cooling factor. The optimal values for these parameters depend on the specific objective function being minimized. Given this, Marxan offers the option of setting these parameters adaptively based on the objective function. In the language of Marxan, this is referred to **adaptive annealing** in contrast to **fixed annealing** in which the users sets these parameters directly.

Marxan chooses sensible values for these parameters by iteratively flipping the status of one planning unit at a time, calculating the resulting change in objective function, and keeping track of the maximum and minimum size of "bad" changes (i.e. those that increased the objective function). The initial temperature is set to the maximum bad change and the final temperature is set to 10% of the minimum bad change. The cooling factor is then inferred from the initial and final temperatures. 

```{r adaptive}
adaptive_schedule <- function(n, objective, nsf, x_init) {
  epsilon <- 1e-10
  scores <- numeric(n)
  x <- x_init
  scores[1] <- objective(x)
  for (i in 2:n) {
    x <- nsf(x)
    scores[i] <- objective(x)
  }
  delta <- diff(scores)
  delta <- delta[delta > epsilon]
  c(t_initial = max(delta), t_final = 0.1 * min(delta))
}
# Rprof("adaptive.out")
# adaptive_schedule(1000, ex_objective, ex_nsf, init_res(pu, 0.5))
# Rprof(NULL)
# summaryRprof("adaptive.out")
# system.time(adaptive_schedule(1000, ex_objective, ex_nsf, init_res(pu, 0.5)))
```

## Marxan!

All the pieces are now in place and it's time to build the Marxan emulator!

```{r marxan-emulator}
marxan_emulator <- function(pu, feature_stack, target_prop,
                            spf = 1, blm = 0, init_prop = 0,
                            nrep = 1, n = 10000, ntemp = n / 10) {
  # representation level of features in planning units
  rij <- summarize_features(feature_stack, pu)
  # set targets and spf
  features <- prepare_features(rij, target_prop, spf)
  # calculate boundaries
  boundary <- calculate_boundary(pu)
  # neighbour function
  nsf <- generate_nsf(pu)
  # objective function
  objective <- generate_objective(pu, rij, features, boundary, blm)
  # adaptive annealing, find annealing parameters
  temp <- adaptive_schedule(max(1000, n / 100), objective, nsf,
                                x_init = init_res(pu, prop = init_prop))
  alpha <- unname(exp(log(temp["t_final"] / temp["t_initial"]) / ntemp))
  results <- data.frame(run = integer(), pu = integer(), x = logical())
  # run simulated annealing for multiple repetitions to get a suite of
  # candidate reserve networks to analyze
  for (i in 1:nrep) {
    x_result <- anneal(x_init = init_res(pu, prop = init_prop),
           objective = objective, neighbour = nsf, n = n, 
           ntemp = ntemp, t_init = temp["t_initial"], alpha = alpha)
    results <- rbind(results, data.frame(run = i, pu = pu$id, x = x_result))
  }
  list(pu = pu, features = features, boundary = boundary,
       objective = objective, nsf = nsf,
       params = c(temp, alpha = alpha, nrep = nrep, n = n, ntemp = ntemp),
       blm = blm,
       selections = results)
}
```

# Reserve selection

Now that I've created an R function that emulates Marxan, it's time to put it to the test. I'll compare the results of reserve selection exercises using Marxan and R with identical parameters. Since there is inherent stochasticty in simulated annealing, the results won't be identical, but they should be quite similar if I've implemented everything correctly.

## Preparations

In generating the species distributions at the start of this tutorial, I created two levels of rarity: species cover 10% or 50% of the study area. Working under the assumption that rare species need more of their range protected, I set percent representation targets for species in the different rarity categories at 25%, 15%, and 10%, respectively.

```{r set-targets}
target_prop <- data_frame(feature = letters[1:4], prop = c(0.5, 0.4, 0.3, 0.2))
kable(target_prop)
```

The [Marxan Good Practices Handbook](http://www.uq.edu.au/marxan/docs/Marxan%20Good%20Practices%20Handbook%20v2%202010.pdf) outlines an iterative approach for determining the optimal Species Penalty Factors. Essentially, one runs Marxan many times for different values of the SPF to find the minimum value for each species below which targets stop being met. For this tutorial, I won't delve into calibration of the SPF, rather I've done this outside the tutorial and enter some sensible values here.

```{r set-spfs}
spfs <- data_frame(feature = letters[1:4], spf = c(1, 1, 1, 1))
```

## Emulator

```{r}
system.time(
  results <- marxan_emulator(pu, species, target_prop, spfs,
                             blm = 1, nrep = 1, n = 1000)
)
```

## Marxan
I start with a normal Marxan run, which I will eventually compare to my R emulation. The R package `marxan` makes running Marxan from within R a breeze

```{r mardan-data}
mr <- marxan(pu, species,
             targets =  results$features$target,
             spf = c(7, 7, 12, 17),
             BLM = 20, PROP = 0, VERBOSITY = 1L,
             NUMREPS = 100L, NUMITNS = 100000L, NUMTEMP = 10000L)
mr@results@mpm
plot(mr, 1)
plot(mr)
mr@results@summary %>% View
hist(mr@results@summary$Cost)


blm_runs <- geom_ramp(1, 100, n = 8, inc_zero = T) %>% 
    calibrate_blm(mr, blm = ., spf = c(7, 7, 10, 10),
                  verbose = F, n_reps = 100)

library(ggplot2)
library(scales)
blm_runs %>% 
  group_by(blm_label) %>% 
  summarize_each(funs(mean), cost, boundary_length) %>% 
  ggplot(aes(x = boundary_length, y = cost)) + 
    geom_point(aes(color = blm_label, size = 3)) +
    #geom_line() +
    scale_color_brewer(name = 'BLM', palette = 'Set1') +
    scale_x_continuous(labels = comma) +
    guides(size = FALSE) +
    labs(x = 'Boundary Length', y = 'Cost') +
    theme_bw()

data(taspu, tasinvis)
mr_tas <- marxan(taspu, tasinvis, targets="20%", spf=100, NUMREPS=100L,
       NCORES=2L, BLM=0.0001, lengthFactor=1e-5)

blm_runs %>% 
  group_by(blm_label) %>% 
  mutate(cost_prop_best = 100 * cost / min(cost),
         n_feasible = sum(mpm > 0.95)) %>% 
  ggplot(aes(x = cost_prop_best, color = blm_label)) +
    geom_step(aes(n=n_feasible, y = (..y.. * n)), stat="ecdf") +
    labs(x = 'Cost (% of best solution)',
         y = 'Cumulative # Solution Meeting 95% of Targets') +
    scale_color_brewer(name = 'BLM', palette = 'Set1') + 
    theme_bw()


```

```{r}
calibrate_blm <- function(x, blm, spf, n_reps, verbose = F) {
  # Test a range of Boundary Length Modifiers (BLMs) to find optimal value
  #
  # Args:
  #   x: MarxanUnsolved or MarxanSolved object
  #   blm: numeric; vector of BLMs to test
  #   spf: numeric; spf to be held fixed while spf is varied, if no arguement provided, 
  #           will default to values from x
  #   n_reps: integer; number of annealing repititions, if no arguement provided, 
  #           will default to values from x
  #   verbose: logical; whether to print progress
  #
  # Returns:
  #   data frame
  
  if (!missing(spf)) {
    if (length(spf) == length(x@data@species$spf)) {
      x@data@species$spf <- spf
    } else {
      x@data@species$spf <- rep(spf, length(x@data@species$spf))
    }
  }
  
  if (!missing(n_reps)) {
    x@opts@NUMREPS <- as.integer(n_reps)
  }
  
  if (verbose) {
    x@opts@VERBOSITY <- 2L
  } else {
    x@opts@VERBOSITY <- 0L
  }
  
  run_marxan <- function(b, x) {
    if (verbose) {
      print(paste0('BLM = ', b))
    }
    x@opts@BLM <- b
    soln <- solve(x, force_reset = T)
    soln@results@summary %>% 
      dplyr::select(run_num = Run_Number,
             score = Score,
             cost = Cost,
             # obj func penalty for not meeting targets
             penalty = Penalty,
             n_pu = Planning_Units,
             boundary_length = Connectivity,
             # sum of gaps between actual representation levels and targets
             shortfall = Shortfall,
             # number of features for which a target was not met
             n_missed = Missing_Values,
             # lowest proportional target achievement among features
             mpm = MPM)
  }
  results <- data.frame(blm = blm) %>% 
    group_by(blm) %>% 
    do(run_marxan(.$blm, x = x)) %>% 
    ungroup
  if (max(results$blm) < 9999 && min(results$blm > 0.1)) {
    results$blm_label <- factor(round(results$blm, 2))
  } else {
    results$blm_label <- factor(format(results$blm, scientific = T, digits = 3))
  }
  results <- mutate(results, blm_label = reorder(blm_label, blm, min))
  return(results)
}
geom_ramp <- function(mn, mx, n = 10, inc_zero = F) {
  if (mn <= 0 | mn >= mx) {
    stop('must have 0 < mn < mx for exponential ramp.')
  }
  if (inc_zero) {
    n = n - 1
  }
  delta <- log(mx / mn) / (n - 1)
  ramp <- mn * exp(delta * (1:n - 1))
  if (inc_zero) {
    ramp <- c(0, ramp)
  }
  return(ramp)
}
```

